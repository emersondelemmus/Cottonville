\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
%\usepackage{natbib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{dblfloatfix} 
\usepackage{afterpage}
\usepackage{placeins}
\usepackage{comment}
\usepackage{caption}
\usepackage{tabto}
\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage{float}
\usepackage{siunitx}
\usepackage{dcolumn}
\renewcommand\tabularxcolumn[1]{m{#1}}
\hypersetup{
    colorlinks=false,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\graphicspath{Images/}




\begin{document}

\title{Real-Time Classification of Cotton Diseases with a Mobile App using Transfer Learning \\}

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother


\author{
\IEEEauthorblockN{Emerson A. de Lemmus,
Brendan P. McAntosh, 
ABM Rezbaul Islam Ph.D., and
Kolby T. Stafford}
\IEEEauthorblockA{Department of Computer Science,
Sam Houston State University\\
United States\\
}}



\maketitle

\begin{abstract}
Cotton, a crucial crop in Texas, accounting for a third of US production, often suffers significant losses due to diseases. This paper introduces a proof-of-concept mobile application employing MobileNetV3 and NASNetMobile transfer learning models to detect cotton plant diseases. The system addresses a four-class problem, identifying the health status of cotton plants or leaves. Using transfer learning and TensorFlow, these models were trained on public data, and an iOS app served as the system's deployment vehicle. We also evaluated the performance of the two state-of-the-art, unused in this context, neural networks - MobileNetV3 and NASNetMobile. Our methodology, demonstrating a promising potential for practical applications, achieved overall accuracies of 97.7\% for NASNetMobile and 96.7\% for MobileNetV3.
\end{abstract}

\begin{IEEEkeywords}
 Cotton Disease, Transfer Learning, MobileNetV3, NASNetMobile, Real-time detection, Mobile Application
\end{IEEEkeywords}

\section{Introduction}
According to the USDA's National Agriculture Statistics Service (NASS), Texas produced 6.32 million 480-pound bales of cotton in 2019, making it the top cotton-producing state in the country, accounting for approximately 33\% of the nation's total cotton production \cite{ USDA-NASS}. However, cotton is susceptible to various diseases, with 80-90\% of them, such as fungi and foliar leaf spots, manifesting on the cotton leaves \cite{Gulhane-Gurjar}. There is a demand for an accurate, affordable machine vision system to enable disease detection, which is crucial for optimal cotton growth and yield. Manual disease identification is time-consuming and labor-intensive; automating the process can enhance efficiency and provide clear disease verification for agricultural experts.

The deep learning (DL) revolution has significantly impacted the AI industry in recent years. Since its 2012 victory over traditional machine learning algorithms in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), DL has become the leading solution for image recognition \cite{Ashqar-Naser}\cite{Gehlot-Saini}. Convolutional Neural Networks (CNNs) excel in object identification and image classification \cite{Sarangdhar-Pawar}. Composed of multiple layers of artificial neurons, each layer in a CNN is responsible for detecting different image features, such as edges and shapes. The final layer outputs a probability distribution over the potential image classes.

Transfer learning, a popular approach in deep learning, leverages pre-trained CNNs for computer vision tasks instead of building a model from scratch \cite{Brownlee}. This technique reuses knowledge from one task for another, effectively addressing data scarcity and computational resource limitations in various applications, including computer vision, natural language processing, and speech recognition. By retraining a CNN model with thousands of high-quality images, it can identify and classify new objects. This paper investigates the use of transfer learning as a resource-efficient method to create accurate classification models for providing essential plant health information to agricultural experts, as it has proven effective in detecting diseases in various crops \cite{Disease Detection} \cite{Disease Diagnosing}.

This paper demonstrates a proof-of-concept system for detecting diseased cotton plants and leaves using transfer learning with two state-of-the-art models which have not been used in this problem space before. A performance comparison is made by retraining the weights of two popular mobile device models, MobileNetV3 (2019) and NASNetMobile (2018) \cite{WandB}. During this study in Spring 2019, both models were state-of-the-art, had similar performances, and are optimized for mobile/edge devices. Importantly, they are models not previously used in edge devices for foliage disease classification. A mobile app serves as the visual data input, classifying foliage as diseased or non-diseased with a trained MobileNetV3 model, instantly informing users of the plant's health. The study employs a four-classifier problem, with labels for healthy and diseased cotton leaves and plants. Publicly available data \cite{Kaggle} is used to train the MobileNetV3-Small and NASNetMobile models, optimized for mobile environments. TensorFlow Lite transfers the retrained models to mobile devices, offering an accessible and straightforward implementation of transfer learning for agricultural advancements.

\section{RELATED WORKS}

Numerous studies demonstrate the effectiveness of deep learning in plant disease detection and classification. For instance, Singh et al. \cite{Singh-etal} and Mohanty et al. \cite{Mohanty-etal} achieved detection accuracies of 96.9\% and 98.3\% for rice and cassava diseases, respectively, using CNNs. In cotton disease detection, both Bhoi \cite{Bhoi} and Dey et al.\cite{Dey-etal} utilized deep learning, yielding accuracies above 90\%. Additionally, transfer learning has proven successful in disease detection, with Kazemi et al\cite{Kazemi-etal} and Sharma et al.\cite{Sharma-etal} achieving over 97\% accuracy for tomato and potato diseases, respectively. Mobile applications have also been effectively implemented. For instance, Siddique et al.\cite{Siddique-etal} and Uddin et al.\cite{Uddin-etal} developed mobile apps for wheat and mango disease detection using transfer learning, both achieving over 90\% accuracy. These studies underscore the utility of deep learning, transfer learning, and mobile applications in accessible, practical disease detection for agriculture.

\section{Methodology}

This study aims to create an accessible, efficient, and real-time mobile application for cotton disease detection, utilizing transfer learning. The application uses the mobile phone camera to provide instantaneous health classification of cotton plants or leaves, functioning even in areas with limited internet connectivity. This approach creates a cost-effective, precise, and resource-efficient system. In contrast to traditional disease detection methods, which can be labor-intensive, expensive, and slow, our mobile application can operate offline, providing immediate classification and offering a practical and innovative solution for cotton disease detection. Using MobileNetV3 and NASNetMobile, two previously unused state-of-the-art CNN architectures in foliage disease identification, this study also offers a performance baseline for studies using future iterations of MobileNet and NASNet architectures. 

\subsection{Transfer Learning}

Transfer learning, a machine learning technique, leverages a pre-trained model—often trained on a large dataset like ImageNet—to fine-tune the performance for a specific task \cite{Shu}. This approach capitalizes on the pre-trained model's generalized weights, facilitating swift prototyping of architectures while being cost-effective. For instance, training a model on ImageNet, a 14-million-image dataset \cite{Reynolds}, is prohibitively expensive for most researchers. In 2017, the cost per training iteration was estimated at \$1112.64, excluding upfront hardware costs \cite{genuineimpact}.

As depicted in \emph{Fig. \ref{Transfer Learning}}, transfer learning employs a pre-trained model, often sourced from PyTorch or Tensorflow, while 'freezing' the depthwise and bottleneck layers, where the majority of pre-trained weights reside. Preserving these weights is beneficial due to their capacity to discern a variety of features. However, as these weights may not recognize novel targets such as diseased foliage, fine-tuning the adaptive average pooling, fully connected, and output layers is essential. This process equips a model to classify diseased foliage, despite its lack of prior training on such data.

\begin{figure}[h]
\centerline{\includegraphics[height=6.5cm, width=1\linewidth]{Images/MobileNet_Small.png}}
\caption{Visualization of transfer learning implementation in a CNN inspired by MobileNetV3. The diagram highlights the preservation of convolutional layers to utilize their wide feature discernment ability, with emphasis on fine-tuning the pooling and output layers for the specific task of cotton foliage disease identification.}
\label{Transfer Learning}
\end{figure}

\subsection{Dataset}
In this study, we utilized a dataset sourced from Kaggle \cite{Kaggle} that is publicly available. It comprises 2,310 labeled images of cotton plants and leaves. These images are classified into four categories: healthy cotton leaves, diseased cotton leaves, healthy cotton plants, and diseased cotton plants as displayed in \emph{Fig. \ref{CottonImages}}. The dataset includes 1764 training, 440 validation, and 106 testing images. All images are in JPG format and RGB color space, each with a resolution of 224 x 224 pixels. These images were collected under similar conditions including lighting, angles, and stages of disease progression. In future works, this effort will benefit from having an enriched dataset that accounts for multiple disease categories and includes enriched conditions. We divided the training dataset into an 80:20 ratio for training and validation, respectively. Both MobileNetV3 and NASNetMobile were trained to identify specific features indicative of cotton foliage health, with their performance evaluated against the validation set. Our objective is to construct an accurate and efficient prototype application for cotton disease detection using transfer learning. Using this dataset, we can compare the baseline performance of mobile device models and assess their applicability in the agricultural sector, specifically for real-time disease detection in cotton plants.

\begin{figure}[h]
\centerline{\includegraphics[height=8.5cm, width=.9\linewidth]{Images/cotton images.drawio.png}}
\caption{Sample images for each class featured in the dataset. Image credit: B. Janmejay.}
\label{CottonImages}
\end{figure}


\subsection{Data pre-processing}

In this study, we meticulously adhered to a standardized data pre-processing protocol, promoting data compatibility across all models under examination. This essential step in data mining refines raw data into an analyzable format, curbing unnecessary distortions and accentuating key image features. Initially, we resized images to a consistent 224 x 224 pixel dimension, aligning with the input requirements of both MobileNetV3 and NASNetMobile, ensuring no feature loss or artifact generation. Subsequently, we implemented data augmentation, a strategy that amplifies the dataset by generating slightly altered copies of existing images, thereby bolstering the models' resilience. This approach entailed image transformations such as rotation, horizontal-vertical flipping, and alterations in width, height, shear, zoom, and pixel values, depicted in \emph{Fig. \ref{Augment}}. Preserving realistic scenarios, we retained the original RGB color space, opting against grayscale conversion. Each training image was augmented to produce five unique images, thereby expanding the training dataset from 1763 to 8816 images. Contrarily, the validation subset, comprising 440 images, remained untouched to maintain the validity of our results, as data augmentation may inadvertently introduce biases and artifacts. Our adherence to a standardized pre-processing approach minimized potential biases and ensured comparability of our results. This structured methodology is integral to our goal of developing a reliable and valid classification model for cotton disease detection.
 
\begin{figure*}[ht]
\centering
\includegraphics[height=3.2cm, width=1\linewidth]{Images/Data_Augmentation2_LI (2).pdf}
\caption{Example of data augmentation techniques applied to an image: a raw image undergoes various transformations, such as rotation, scaling, flipping, and shearing, to increase the diversity of the training dataset and improve model generalization.}
\label{Augment}
\end{figure*}


\subsection{Defining Convolutional Neural Networks}

Convolutional Neural Networks (CNNs), comprising sequential layers for processing input images, are a potent tool for image classification. The process begins with the first layer applying multiple convolution operations to the input image, generating a feature map matrix. This matrix undergoes dimension reduction through resizing as it progresses through the layers. The output from one layer serves as input to the subsequent layer, culminating in a fully connected layer that compacts all features into a vector for the final softmax layer, which in turn outputs the classifier labels and their associated probabilities.

Mathematically, the convolution operation performed in CNNs' initial layers can be expressed as:

\begin{equation}
Z(i,j) = (I * K)(i,j) = \sum_m \sum_n I(m,n) K(i-m,j-n)
\end{equation}

Here, $I$ is the input image, $K$ the filter kernel, and $Z(i,j)$ the output feature map matrix. The filter is applied to each element of the input image, with the resultant values summed to form each output matrix element.

Each layer's output is computed via:

\begin{equation}
A^{(l)} = g(Z^{(l)}) = g(W^{(l)}A^{(l-1)} + b^{(l)})
\end{equation}

In this equation, $W^{(l)}$ and $b^{(l)}$ denote the weights and biases of the $l$-th layer respectively, $A^{(l-1)}$ is the input feature map, $g$ represents the activation function, and $Z^{(l)}$ is the weighted sum of input features.

The final softmax layer computes classifier labels as follows:

\begin{equation}
P(y=j|x) = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}
\end{equation}

Here, $P(y=j|x)$ is the probability of classifying the image as label $j$, $z_j$ refers to the weighted sum of features for label $j$, and $K$ is the total number of labels \cite{Smeda}.


\subsection{MobileNetV3}

The first model chosen for this study is MobileNetV3 - Large, an advanced mobile convolutional neural network (CNN) introduced by Google in Q2 2019, specifically designed for mobile and embedded vision applications \cite{Howard-Sandler}. MobileNetV3 leverages depth-wise separable convolutions to significantly reduce computation by a factor of nine, compared to traditional models \cite{Yanhui}. paperswithcode summarizes the MobileNetV3 Large and Small models \cite{paperswithcode}. The large version of the model was chosen for this study as it contains more trainable parameters, has a higher baseline accuracy than the smaller model, and closely resembles NASNetMobile in terms of Parameters, FLOPS, and File Size. \emph{Table \ref{table:MobileNetV3}} and \emph{Table \ref{table:MobileNetV3-Architecture}} summarize MobileNetV3 parameters and architecture concisely \cite{paperswithcode}. With its optimization for devices possessing limited computational resources, this architecture is an ideal choice for deploying image models for mobile applications \cite{Howard-Zhu}.

\begin{table}[ht]
\centering
\caption{MobileNetV3 - Parameters}
%\begin{tabular}{l|l}
\begin{tabularx}{1\columnwidth}{X|>{\centering\arraybackslash}X}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Parameters & 5 Million \\
FLOPS & 225 Million \\
File Size & 21.11 MB \\
Training Data & ImageNet \\
Training Resources & 8x NVIDIA V100 GPUs \\
\hline
\end{tabularx}
\label{table:MobileNetV3}
\end{table}

\begin{table}[ht]
\centering
\caption{MobileNetV3 - Architecture}
\begin{tabularx}{\columnwidth}{>{\centering\arraybackslash}X}
\hline
\begin{tabular}{c}
1x1 Convolution \\
Batch Normalization \\
Convolution \\
Dense Connections \\
Depthwise Separable Convolution \\
Dropout \\
Global Average Pooling \\
Hard Swish \\
Inverted Residual Block \\
Residual Connection \\
ReLU \\
Softmax \\
Squeeze-and-Excitation Block \\
\end{tabular} \\
\hline
\end{tabularx}
\label{table:MobileNetV3-Architecture}
\end{table}



\subsection{NASNetMobile}
The second model chosen for this study is NASNetMobile an advanced mobile CNN also introduced by Google in 2018. NASNetMobile is essentially a neural architecture search algorithm developed by Google Brain for the 2018 ImageNet Competition, which uses reinforcement learning to determine the optimal combination of various hyperparameters. The architecture of NASNetMobile is based on a series of "normal" and "reduction" cells, with each cell comprising multiple blocks that perform a set of operations, including convolution, batch normalization, and activation. The output of each block is computed as:

\begin{equation}
H_{l,k} = F_{l,k}(H_{l-1,1},H_{l-1,2},...,H_{l-1,n})
\end{equation}

where $F_{l,k}$ is the function applied by the $k$-th block in the $l$-th cell, $H_{l-1,1},H_{l-1,2},...,H_{l-1,n}$ are the inputs to the $k$-th block from the previous cell, and $H_{l,k}$ is the output of the $k$-th block in the $l$-th cell. The final output of the NASNetMobile architecture is a softmax layer, which computes the probabilities of each class label given the input image.

NASNetMobile has been designed for use in embedded systems with limited computational resources. The architecture achieves state-of-the-art performance on the ImageNet dataset. Our evaluation of NASNetMobile for cotton disease detection provides insights into the effectiveness of this architecture for crop monitoring applications. \emph{Table \ref{table:NASNetMobile}} \emph{Table \ref{table:NASNetMobile-Architecture}} summarize NASNetMobile parameters and architecture concisely \cite{paperswithcodenas}.

\begin{table}[ht]
\centering
\caption{NASNetMobile - Parameters}
%\begin{tabular}{l|l}
\begin{tabularx}{1\columnwidth}{X|>{\centering\arraybackslash}X}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Parameters & 4 Million \\
FLOPS & 325 Million \\
File Size & 16.92 MB \\
Training Data & ImageNet \\
Training Resources & 8x NVIDIA V100 GPUs \\
\hline
\end{tabularx}
\label{table:NASNetMobile}
\end{table}

\begin{table}[ht]
\centering
\caption{NASNetMobile - Architecture}
\begin{tabularx}{\columnwidth}{>{\centering\arraybackslash}X}
\hline
\begin{tabular}{c}
1x1 Convolution \\
Batch Normalization \\
Convolution \\
Depthwise Separable Convolution \\
Dropout \\
Global Average Pooling \\
Inverted Residual Block \\
Residual Connection \\
ReLU \\
Max Pooling \\
Softmax \\
Squeeze-and-Excitation Block \\
\end{tabular} \\
\hline
\end{tabularx}
\label{table:NASNetMobile-Architecture}
\end{table}

%\begin{table}[ht]
%\centering
%\caption{NASNetMobile - Architecture}
%\begin{tabularx}{\columnwidth}{>{\centering\arraybackslash}X}
%\hline
%1x1 Convolution \\
%Batch Normalization \\
%Convolution \\
%Depthwise Separable Convolution \\
%Dropout \\
%Global Average Pooling \\
%Inverted Residual Block \\
%Residual Connection \\
%ReLU \\
%Max Pooling \\
%Softmax \\
%Squeeze-and-Excitation Block \\
%\hline
%\end{tabularx}
%\label{table:NASNetMobile-Architecture}
%\end{table}






\section{Experimental Results}

Performance evaluation of both models relied on metrics such as accuracy, recall, precision, and F1-score, as shown in \emph{Table \ref{table:MNCM}}, \emph{Table \ref{table:MNPerformance}}, \emph{Table \ref{table:NNMCM}}, and \emph{Table \ref{table:NNMPerformance}}. Despite uniform data pre-processing and a 20-epoch training phase, the Google Colab-free environment imposed epoch limitations due to its slower processing speed compared to onsite hardware.


In \emph{Table \ref{table:MNParam}} and \emph{Table \ref{table:NNMParam}} are the hyperparameters for each model. 

\begin{table}[htbp]
    \centering
    \caption{MobileNetV3 - Hyperparameters}
    \begin{tabularx}{1\columnwidth}{X|>{\centering\arraybackslash}X}
    \hline
    \multicolumn{1}{c|}{\textbf{Parameter}} & \multicolumn{1}{c}{\textbf{Value}} \\
    \hline
    Learning Rate & 0.064 \\
    Epochs & 20 \\
    LR Gamma & 0.973 \\
    Momentum & 0.9 \\
    Batch Size & 128 \\
    LR Step Size & 2 \\
    Random Erase & 0.2 \\
    Weight Decay & 0.00001 \\
    \hline
    \end{tabularx}
    \label{table:MNParam}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{NASNetMobile - Hyperparameters}
    \begin{tabularx}{1\columnwidth}{X|>{\centering\arraybackslash}X}
    \hline
    \multicolumn{1}{c|}{\textbf{Parameter}} & \multicolumn{1}{c}{\textbf{Value}} \\
    \hline
    Learning Rate & 0.1 \\
    Epochs & 20 \\
    LR Gamma & 0.1 \\
    Momentum & 0.9 \\
    Batch Size & 32 \\
    LR Step Size & 30 \\
    Weight Decay & 0.0001 \\
    \hline
    \end{tabularx}
    \label{table:NNMParam}
\end{table}

\subsection{Results -  MobileNetV3}

Based on the experimental results, the training loss and validation loss were observed to be 72.90\% and 75.92\%, respectively, after 20 epochs of training. The best validation accuracy of 98.66\% and the best training accuracy was 99.29\%. These findings indicate the effectiveness of the proposed model in accurately classifying cotton foliage into the four categories of healthy cotton leaves, diseased cotton leaves, healthy cotton plants, and diseased cotton plants. The plots in \emph{Fig. \ref{MNStats}.} visualize these statistics. 

\begin{figure}[h]
\centerline{\includegraphics[height=10cm, width = .9\linewidth]{Images/Screen_Shot_2021-05-04_at_5.27.06_PM.pdf}}
\caption{MobileNetV3 trainig performance over 20 epochs. \centering{\newline Top: Plots training and validation loss against epochs. \newline Bottom: Plots accuracy and validation accuracy against epochs.}}
\label{MNStats}
\end{figure}


CNNS are prevalent in image classification, with the relationship between training and validation loss often intricate. A model overfits if the validation loss considerably exceeds the training loss, while it underfits if the training loss either plateaus or declines without a corresponding decrease in validation loss. This study observed an optimal fit, with both losses descending to a stable point with a minimal gap.

For assessing system accuracy and robustness, 106 test images were deliberately isolated from the training dataset, providing diverse, unseen data for performance evaluation. The test phase allowed for an appraisal of the model's generalization ability, crucial for real-world applications. The resulting test data provided a reliable estimate of system accuracy and reliability, thereby demonstrating its efficacy in cotton disease detection. This data further facilitated the generation of a confusion matrix displayed in \emph{Table \ref{table:MNCM}}.

\begin{table}[htbp]
    \centering
    \caption{MobileNetV3 - Confusion Matrix}
    \label{table:MNCM}
    \begin{tabularx}{0.49\textwidth }{ 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X
        }
    \hline
    \textbf{Class (images)} & \textbf{Fresh Leaf} & \textbf{Fresh Plant} & \textbf{Diseased Leaf} & \textbf{Diseased Plant} \\
    \hline 
    Fresh Leaf (26) & 24 & 0 & 2 & 0 \\
    Fresh Plant (27) & 0 & 22 & 0 & 5 \\
    Diseased Leaf (25) & 0 & 0 & 24 & 1 \\
    Diseased Plant (28) & 0 & 0 & 0 & 28 \\
    \hline
    \end{tabularx} 
\end{table}

The overall accuracy of the proposed model is given by equation \ref{eq:oa}:
\begin{equation}
Overall Accuracy = \frac{TP + TN}{TP+TN+FP+FN}
\label{eq:oa}
\end{equation}

where TP represents true positive, TN represents true negative, FP represents false positive, and FN represents false negative.\newline


Our trained MobileNetV3 model achieved high accuracy rates for each label; the overall accuracy of the model was 96.7\% These results demonstrate the effectiveness of our model in accurately identifying and classifying the health status of cotton foliage. Notably, the fine-tuned model performed the lowest when classifying fresh plants. The reason for this inconsistency is the visual similarity between a fresh cotton plant and a diseased cotton plant as demonstrated in \emph{Fig. \ref{Diseased_Fresh_Plant}}, it can be challenging to differentiate even with human vision.

\begin{table}[htbp]
    \centering
    \caption{MobileNetV3 - Performance Metrics}
    \label{table:MNPerformance}
    \begin{tabularx}{0.49\textwidth }{ 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X
        >{\centering\arraybackslash}X
        }
    \hline
    \textbf{Class} & \textbf{Recall} & \textbf{Precision} & \textbf{F1-Score} & \textbf{Accuracy} \\
    \hline 
    Fresh Leaf & 0.923 & 1.000 & 0.960 & 97.5\% \\
    Fresh Plant & 1.000 & 0.815 & 0.898 & 94.8\% \\
    Diseased Leaf & 0.960 & 0.960 & 0.960 & 97.5\% \\
    Diseased Plant & 1.000 & 1.000 & 1.000 & 100\% \\
    \hline
    \end{tabularx} 
\end{table}

\begin{figure}[h]
\centerline{\includegraphics[height=5cm, width=1\linewidth]{Images/Disease_FreshComparison.drawio.png}}
\caption{Here are two randomly selected images from the Kaggle dataset, illustrating the occasional visual similarity between different classes. It is difficult to distinguish the difference - even to the untrained human eye. This attribute contributes to the lower F1-Score (0.898) and Accuracy (94\%) for the Fresh Plant class when using MobileNetV3 for inference. Image credit: B. Janmejay.}
\label{Diseased_Fresh_Plant}
\end{figure}


\subsection{Results - NASNetMobile}

A similar process was performed for NASNetMobile. After a training iteration for 20 epochs, the resulting training loss and validation loss were observed to be 75.48\% and 77.41\%, respectively. The best validation accuracy of 98.77\% and the best training accuracy was 99.83\%. These findings indicate NASNetMobile and MobileNetV3 performed very similarly throughout the training process despite MobileNetV3 being Google's latest model and having 1 million more trainable parameters than NASNetMobile. 


\begin{figure}[h]
\centerline{\includegraphics[height=10cm, width = .9\linewidth]{Images/Screen Shot 2021-05-10 at 4.36.35 PM.pdf}}
\caption{NASNetMobile training performance over 20 epochs. \centering{\newline Top: Plots of training and validation loss against epochs. \newline Bottom: Plots of accuracy and validation accuracy against epochs.}}
\label{NNMStats}
\end{figure}

NASNetMobile's training performance, depicted in \emph{Fig. \ref{NNMStats}}, exhibited superior results compared to MobileNetV3, demonstrating a better data fit. Both models achieved an optimal fit, but NASNetMobile's enhanced performance is attributed to the automated neural architecture search (NAS) algorithm. NAS automatically adapts optimal architecture, including hyperparameters, filter sizes, output channels, and convolution layer count, thereby ensuring tailored performance for the given dataset. This suggests NASNetMobile's potential utility in developing effective CNNs across various sectors, including agriculture \cite{Yanhui}. The same 106 test images used for MobileNetV3 were also isolated to evaluate NASNetMobile's accuracy and robustness. It is imperative to note that high training metrics don't assure optimal test performance. NASNetMobile's confusion matrix is displayed in \emph{Table \ref{table:NNMCM}}.

\begin{table}[htbp]
    \centering
    \caption{NASNetMobile - Confusion Matrix}
    \label{table:NNMCM}
    \begin{tabularx}{0.49\textwidth }{ 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X
        }
    \hline
    \textbf{Class (images)} & \textbf{Fresh Leaf} & \textbf{Fresh Plant} & \textbf{Diseased Leaf} & \textbf{Diseased Plant} \\
    \hline 
    Fresh Leaf (26) & 26 & 0 & 0 & 0 \\
    Fresh Plant (27) & 1 & 25 & 0 & 1 \\
    Diseased Leaf (25) & 2 & 0 & 23 & 0 \\
    Diseased Plant (28) & 0 & 0 & 0 & 28 \\
    \hline
    \end{tabularx} 
\end{table}

The trained NASNet model delivered high accuracy for each label, achieving an overall accuracy of 97.7\%, outperforming MobileNetV3's 96.7\%. Despite being older and smaller, NASNetMobile outclassed MobileNetV3 in all categories, as demonstrated in \emph{Table \ref{table:NNMPerformance}}. It exhibited consistent performance across all classes, overcoming MobileNetV3's classification challenge with Fresh Cotton Plants. Though differences are slight, this study underscores the superiority of the NAS algorithm over models lacking it.


\begin{table}[htbp]
    \centering
    \caption{NASNetMobile - Performance Metrics}
    \label{table:NNMPerformance}
    \begin{tabularx}{0.49\textwidth }{ 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X 
        >{\centering\arraybackslash}X
        }
    \hline
    \textbf{Class} & \textbf{Recall} & \textbf{Precision} & \textbf{F1-Score} & \textbf{Accuracy} \\
    \hline 
    Fresh Leaf & 1.000 & 0.897 & 0.945 & 98.8\% \\
    Fresh Plant & 0.926 & 0.962 & 0.944 & 96.1\% \\
    Diseased Leaf & 0.920 & 0.920 & 0.920 & 97.7\% \\
    Diseased Plant & 1.000 & 1.000 & 1.000 & 100\% \\
    \hline
    \end{tabularx} 
\end{table}

\section{Mobile Application}

The mobile application, developed on the iOS platform using TensorFlow Lite, incorporates both MobileNetV3 and NASNetMobile for real-time classification, as shown in \emph{figure \ref{DiseasedCottonLeaf}}. During the testing phase, all 106 test images were scanned using this application on an iPhone 12 Pro Max. The absence of an Android version, due to the lack of a comprehensive solution for model bundling and loading, will be addressed in the future developments section. Future enhancements aim at user interface and experience improvements within a shorter development timeline. The compact mobile application, with each model not exceeding 21.11 MB, provides a portable solution for cotton disease classification, presenting significant potential impact on the agricultural industry.

\begin{figure}[h]
\centerline{\includegraphics[height=11.5cm, width =1\linewidth]{Images/Diseased Cotton Leaf.png}}
\caption{An example of classifying diseased cotton leaf via iOS mobile application.}
\label{DiseasedCottonLeaf}
\end{figure}

\section{Future Developments}

\subsection{Dataset Expansion}

The Kaggle dataset used in this study, while sufficient for a proof-of-concept, lacks the comprehensive and robust nature necessary for a thorough disease detection system. Ideally, the dataset would incorporate specific disease labels for both foliar and root ailments, facilitating more nuanced classification beyond the binary healthy/diseased distinction. However, this did not impede our primary objective: to train, via transfer learning, two state-of-the-art models which were previously unused in foliar disease detection and deploy a real-time mobile application. The dataset's limitations also extend to early disease lifecycle detection due to the insufficient data on early-stage diseased plants. The early stage disease manifestation makes it difficult, even for human eyes, to differentiate between healthy/diseased classes. An extensive dataset with expertly annotated disease stages could potentially stimulate new avenues of experimentation.

\subsection{Recommender System}

By expanding the dataset, possibly through merging distinct sources with specific disease labels, a compact recommender system could be implemented, suggesting potential remedies or disease treatments. This strategy, as exemplified by the PlantVillage Nuru system by Pennsylvania State University \cite{PlantVillage}, could particularly benefit underserved communities, delivering an offline, region-specific system demanding minimal updates in regions where cotton cultivation forms a substantial income source.

\subsection{Expanding Access}
To broaden our work's reach, we aim to develop a cross-platform native integration for Android and iOS, leveraging a unified design and code base to facilitate additional functionality. However, currently, no efficient tool allows TensorFlow Lite model bundling and loading on both platforms. This constraint identifies a potential area for future development that could simultaneously publish native applications on both platforms. Such advancement would benefit not only our methodology but also similar field experiments, enabling wider reach and increased impact.


\section{Conclusion}

This study highlights the simplicity and efficiency of transfer learning for training two advanced mobile-compatible models, NASNetMobile and MobileNetV3. By utilizing transfer learning, we leverage pre-trained weights superior at image feature detection compared to models trained from scratch. Fine-tuning these models yielded 97.7\% and 96.7\% accuracy respectively, demonstrating NASNetMobile's superior performance, despite having fewer trainable parameters and being older. Notably, NASNetMobile excelled in identifying fresh cotton plants, a task challenging for MobileNetV3, likely due to NASNetMobile's self-updating architecture during training. The straightforward deployment of these models on an iOS application via TensorFlow Lite underscores the feasibility of this process, requiring only basic hardware and no upfront costs.


%\bibliography{references}

    
\begin{thebibliography}{00}

\bibitem{USDA-NASS} National Agricultural Statistics Service, "Annual Cotton Review," [Online] Available: \href{https://www.nass.usda.gov/Statistics_by_State/Texas/Publications/Current_News_Release/2020_Rls/tx-cotton-review-2020.pdf}{nass.usda.gov}, 2019.

\bibitem{Gulhane-Gurjar} V. A. Gulhane and A. A. Gurjar, "Detection of Diseases on Cotton Leaves and Its Possible Diagnosis," \textit{International Journal of Image Processing (IJIP)}, vol. 5, no. 5, 2011.

\bibitem{Ashqar-Naser} Belal A.M. Ashqar, Samy S. Abu-Naser, "Image-Based Tomato Leaves Diseases Detection Using Deep Learning," International Journal of Academic Engineering Research (IJAER), Vol. 2, Issue 12, December 2018, pp. 10-16, ISSN: 2000-001X

\bibitem{Gehlot-Saini} Mamta Gehlot and Madan Lal Saini, "Analysis of Different CNN Architectures For Tomato Leaf Disease Classification," 5th IEEE International Conference on Recent Advances and Innovations in Engineering, ICRAIE 2020 (IEEE Record\#51050), 2020. 

\bibitem{Sarangdhar-Pawar} A. A. Sarangdhar and V. R. Pawar, "Machine learning regression technique for cotton leaf disease detection and controlling using IoT," in \textit{2017 International Conference of Electronics, Communications and Aerospace Technology (ICECA)}, 2017, vol. 2, pp. 449-454.

\bibitem{Brownlee} J. Brownlee, "A Gentle Introduction to Transfer Learning for Deep Learning," Machine Learning Mastery, December, 2017. [Online], Available: \href{https://machinelearningmastery.com/transfer-learning-for-deep-learning/#:~:text=Transfer\%20learning\%20is\%20a\%20machine,model\%20on\%20a\%20second\%20task.&text=Common\%20examples\%20of\%20transfer\%20learning,your\%20own\%20predictive\%20modeling\%20problems.}{machinelearningmastery.com}

\bibitem{Disease Detection} O. Kulkarni, "Crop Disease Detection Using Deep Learning," \textit{2018 Fourth International Conference on Computing Communication Control and Automation (ICCUBEA)}, Pune, India, 2018, pp. 1-4, doi: 10.1109/ICCUBEA.2018.8697390.

\bibitem{Disease Diagnosing} H. Park, E. JeeSook and S. Kim, "Crops Disease Diagnosing Using Image-Based Deep Learning Mechanism," \textit{2018 International Conference on Computing and Network Communications (CoCoNet)}, Astana, Kazakhstan, 2018, pp. 23-26, doi: 10.1109/CoCoNet.2018.8476914.

\bibitem{WandB} C. Lepelaars, "The Evolution Of Mobile CNN Architectures," \textit{WandB}, 8 November 2022, [Online], Available: \href{https://wandb.ai/carlolepelaars/mobile_architectures/reports/The-Evolution-Of-Mobile-CNN-Architectures--VmlldzoyMDQ0ODQ}{wandb.ai}

\bibitem{Kaggle} B. Janmejay, "Cotton Disease Dataset," \textit{Kaggle}, 24 September 2020, [Online] Available: \href{https://www.kaggle.com/janmejaybhoi/cotton-disease-dataset/notebooks}{kaggle.com}

\bibitem{Singh-etal} A. Singh, B. Ganapathysubramanian, S. Sarkar, and A. K. Singh, "Deep learning for plant stress phenotyping: Trends and future perspectives," \textit{Frontiers in Plant Science}, vol. 7, p. 1510, 2016.

\bibitem{Mohanty-etal} S. P. Mohanty, D. P. Hughes, and M. Salathé, "Using deep learning for image-based plant disease detection," \textit{Frontiers in Plant Science}, vol. 7, p. 1419, 2016.

\bibitem{Bhoi} J. M. Bhoi, "Identification of diseases in cotton leaves using machine learning," \textit{Research Square}, 2020.

\bibitem{Dey-etal} P. Dey, P. Roy, and S. Dey, "Automated classification of cotton leaf diseases using deep belief network," \textit{Computers and Electronics in Agriculture}, vol. 173, p. 105413, 2020.

\bibitem{Kazemi-etal} S. H. Kazemi, S. Mohsenzadeh, and A. Fooladgar, "A deep transfer learning approach for tomato diseases detection," \textit{Computers and Electronics in Agriculture}, vol. 162, pp. 706-713, 2019.

\bibitem{Sharma-etal} P. Sharma, A. K. Pandey, and M. Singh, "Potato disease classification using transfer learning approach," \textit{Agricultural Research}, vol. 10, no. 1, pp. 57-66, 2021.

\bibitem{Siddique-etal} A. B. Siddique, M. N. Akhtar, and M. Shafique, "Deep Learning Based Plant Disease Recognition using Mobile Devices," \textit{Procedia Computer Science}, vol. 171, pp. 1129-1136, 2020.

\bibitem{Uddin-etal} M. S. Uddin, M. A. H. Bhuiyan, M. R. Islam, and M. Z. Alom, "A deep learning based mobile application for mango disease detection," \textit{Computers and Electronics in Agriculture}, vol. 186, p. 106024, 2021.

\bibitem{Shu} M. Shu, "Deep Learning for image classification on very small datasets using transfer learning," Iowa State University, Summer 2019.

\bibitem{Reynolds} R. Reynolds, "New computer vision challenge wants to teach robots to see in 3D," \textit{NewScientist}, vol. 234, no. 3123, pp. 28-31, Apr. 2017, Available: \href{https://www.newscientist.com/article/2127131-new-computer-vision-challenge-wants-to-teach-robots-to-see-in-3d/}{NewScientist.com}.

\bibitem{genuineimpact} genuineimpact, "Cost to train AI system," genuineimpact, 2021. [Online]. Available: \href{https://i.redd.it/c9lyy4252qia1.png}{genuineimpact.com}.

\bibitem{Smeda} K. Smeda, "Understand the Architecture of CNN," \textit{Towards Data Science}, Oct. 2019. [Online]. Available: \href{https://towardsdatascience.com/understand-the-architecture-of-cnn-90a25e244c7}{Towards Data Science}.


\bibitem{Howard-Sandler} A. Howard, M. Sandler, et al., "Searching for MobileNetV3," in \textit{2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, Seoul, Korea (South), 2019, pp. 1314-1324, doi: 10.1109/ICCV.2019.00140.

\bibitem{Yanhui} C. Yanhui, "From AlexNet to NASNet: A Brief History and Introduction of Convolutional Neural Networks," \textit{Towards Data Science}, 2021. [Online]. Available: \href{https://towardsdatascience.com/from-alexnet-to-NASNet-a-brief-history-and-introduction-of-convolutional-neural-networks-cf63bf3320e1}{towardsdatascience.com}

\bibitem{paperswithcode} paperswithcode, "MobileNet V3 Large", \textit{paperswithcode}, 2021. [Online] Available: \href{https://paperswithcode.com/lib/torchvision/mobilenet-v3}{paperswithcode.com}

\bibitem{Howard-Zhu} A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, "Mobilenets: Efficient convolutional neural networks for mobile vision applications," \textit{arXiv preprint} arXiv:1704.04861, 2017.

\bibitem{paperswithcodenas} paperswithcode, "NASNetMobile", \textit{paperswithcode}, 2021. [Online] Available: \href{https://paperswithcode.com/lib/torchvision/mnasnet-1-0}{paperswithcode.com}


\bibitem{PlantVillage} PlantVillage, "Solutions - Nuru", \textit{PlantVillage PSU}, 2020. [Online]. Available: \href{https://plantvillage.psu.edu/projects}{plantvillage.com}

%\bibitem{Shorten-Khosh} Shorten, C., Khoshgoftaar, T.M. ``A survey on Image Data Augmentation for Deep Learning''. J Big Data 6, 60 (2019). https://doi.org/10.1186/s40537-019-0197-0

%\bibitem{Gulhane-Gurjar} Mr. Viraj A. Gulhane and Dr. Ajay A. Gurjar, ``Detection of Diseases on Cotton Leaves and Its Possible Diagnosis``, International Journal of Image Processing (IJIP), vol. 5., issue: 5, 2011.

%\bibitem{Zoph-Vasudevan} Barret Zoph, Vijay Vasudevan et. al., ``Learning Transferable Architectures for Scalable Image Recognition'', Google Brain, April 2018.

%\bibitem{Simonyan-Zisserman} Karen Simonyan, Andrew Zisserman. ``Very Deep Convolutional Networks For Large-Scale Image Recognition'', Department of Engineering Science, University of Oxford. April 2015. 

%\bibitem{Neurohive} ``VGG16 - Convolutional Network for Classification and Detection''. November 2018. Available at \href{https://neurohive.io/en/popular-networks/vgg16/}{Neurohive.com}


%\bibitem{Progrev} ``How to Prevent Overfitting | Regularization''. Available at \href{https://programming-review.com/machine-learning/overfitting#cross-validation}{programming-review.com}

%\bibitem{Brownlee2} Jason Brownlee. ``How to use Learning Curves to Diagnose Machine Learning Model Performance''. Machine Learning Mastery. February 2019. Available at \href{https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/}{machinelearningmastery.com}

%\bibitem{Stackexchange} ``Validation Accuracy vs Training Accuracy''. Available at \href{https://stats.stackexchange.com/questions/401696/validation-accuracy-vs-testing-accuracy}{stackexchange.com}


%\bibitem{Singh-et-al} Singh, A., Ganapathysubramanian, B., Sarkar, S., & Singh, A. K. (2016). Deep learning for plant stress phenotyping: Trends and future perspectives. Frontiers in plant science, 7, 1510.

%\bibitem{Mohanty-et-al} Mohanty, S. P., Hughes, D. P., & Salathé, M. (2016). Using deep learning for image-based plant disease detection. Frontiers in plant science, 7, 1419.

%\bibitem{Bhoi} Bhoi, J. M. (2020). Identification of diseases in cotton leaves using machine learning. Research Square.

%\bibitem{Dey-et-al} Dey, P., Roy, P., & Dey, S. (2020). Automated classification of cotton leaf diseases using deep belief network. Computers and Electronics in Agriculture, 173, 105413.

%\bibitem{Kazemi-et-al} Kazemi, S. H., Mohsenzadeh, S., & Fooladgar, A. (2019). A deep transfer learning approach for tomato diseases detection. Computers and Electronics in Agriculture, 162, 706-713.

%\bibitem{Sharma-et-al} Sharma, P., Pandey, A. K., & Singh, M. (2021). Potato disease classification using transfer learning approach. Agricultural Research, 10(1), 57-66.

%\bibitem{Siddique-et-al} Siddique, A. B., Akhtar, M. N., & Shafique, M. (2020). Deep Learning Based Plant Disease Recognition using Mobile Devices. Procedia Computer Science, 171, 1129-1136.

%\bibitem{Uddin-et-al} Uddin, M. S., Bhuiyan, M. A. H., Islam, M. R., & Alom, M. Z. (2021). A deep learning based mobile application for mango disease detection. Computers and Electronics in Agriculture, 186, 106024.

% \bibitem{m} M. Abd Elaziz, A. Dahou, N. A. Alsaleh, A. H. Elsheikh, A. I. Saba, and M. Ahmadein, "Boosting COVID-19 image classification using MobileNetV3 and aquila optimizer algorithm," \textit{Entropy}, vol. 23, no. 11, pp. 1383, 2021.




\end{thebibliography}






%\bibliographystyle{abbrv}
%\bibliography{references}


\end{document}
